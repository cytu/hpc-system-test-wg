# HPCTESTS 2023
## First International Workshop on HPC Testing and Evaluation of Systems, Tools, and Software
## https://olcf.github.io/hpc-system-test-wg/hpctests/hpctests2023

## General Chairs
Verónica G. Melesse Vergara (ORNL, USA)
Bilel Hadri (KAUST, Saudi Arabia)
Vasileios Karakasis (NVIDIA, Switzerland) 

## Details
* **When**: Friday, November 17, 2023, 8:30am - 12pm MST, held in conjunction with [SC23](https://sc23.supercomputing.org/).
* **Where**: Room TBD, Colorado Convention Center, Denver, Colorado


## Description

This workshop brings together HPC researchers, practitioners, and vendors from around the globe to present and discuss state-of-the-art HPC system testing methodologies, tools, benchmarks, tests, procedures, and best practices. The increasing complexity of HPC architectures requires a larger number of tests in order to thoroughly evaluate the status of the system after its installation or before a software upgrade is applied to production systems. Therefore, HPC centers and vendors use different methodologies to evaluate their systems during its lifetime, not only at the beginning during the installation and acceptance time, but also regularly during maintenance windows. This workshop will provide a venue to present and discuss the latest HPC system test technologies and methodologies including, but not limited to, tools used for testing, new test suites and benchmarks developed to assist testing efforts, lessons learned from acceptance and regression testing experiences, and evaluations of new hardware and software that showcase testing best practices. 

The event will include a keynote focused on current HPC system testing topics, followed by a series of paper presentations from peer-reviewed accepted submissions, and will conclude with a panel discussion.

## Call for Papers 
### Workshop scope 
The First International Workshop on HPC Testing and Evaluation of Systems, Tools, and Software (HPCTESTS 2023), held in conjunction with SC23 (Denver, CO, USA), will bring together experts from high performance computing (HPC) centers around the globe to present and discuss state-of-the-art HPC system testing methodologies, tools, benchmarks, and best practices. The workshop will encourage submissions that highlight current benchmarks, tests, and procedures utilized to evaluate today’s HPC systems. This event will provide an avenue to showcase newly developed tools and methodologies, as well as those that are being actively designed to allow authors to gather feedback from the community that could help guide their project. As machine learning (ML) and deep learning (DL) become more prevalent workloads, HPC centers must provide a wider range of services and more robust and resilient resources in order to support both traditional HPC and ML/DL workloads. The workshop will also invite submissions that are looking ahead at the post-exascale future of HPC system testing to help the community develop alternate mechanisms that could be used to adapt to the evolving and emerging workloads. The event will invite and welcome international participation from HPC centers, academic institutions, as well as representatives from vendors in the supercomputing space.

In addition to discussing procedures and tools utilized, submissions can describe challenges, lessons learned, and best practices used for regression testing, acceptance testing, and hardware evaluations. Furthermore, the workshop aims to encourage submissions that explore testbed evaluations as a means to gather preliminary results on system readiness to assist system design and deployment efforts.

HPCTESTS 2023 will provide the first technical venue for HPC researchers and practitioners to submit their findings, early works, results, new tools, and more. The workshop encourages innovative works in HPC system testing to improve and rethink the mechanisms used for acceptance and regression testing of HPC systems, their software stack, and their user environment to help the community prepare for the post-exascale era.

Topics of interest include, but are not limited to:
* Testing methodologies and procedures
* Tools and frameworks for regression testing 
* Automation of testing and continuous regression testing and performance monitoring
* Development and utilization of new proxy-applications, benchmarks, and real-applications to evaluate a system’s reliability and usability
* Efforts to improve reproducibility, sustainability, and availability of tests that can be leveraged by the community
* Hardware and component focused testing including but not limited to CPUs (x86, ARM), GPUs, AI specialized hardware, SmartNICS (DPUs), memory, network, and storage at all scales (single server to supercomputers and cloud environments)
* System software, programming languages, and library testing
* Monitoring and analysis of tests results for decision making
* Best practices and lessons learned from acceptance and/or regression testing
* Early anomaly detection of failure using ML approaches
* Evaluation of early hardware testbeds and strategies to develop tests on early hardware
* Specification-driven strategies for applications testing and formalisms (e.g., interoperability, composition)
* Application-driven testing strategies
* AI-assisted test generation for HPC systems and/or applications

## Deadlines
* Paper Deadline: August 10, 2023 AoE
* Author Notification: September 8, 2023 AoE
* Camera-ready: September 29, 2023 AoE

## Agenda
TBA - after CfP closes.
